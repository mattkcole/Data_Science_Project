db[,]$text1 <- iconv(db[,]$text1, from="UTF-8", to="latin1")
db[,]$text2 <- iconv(db[,]$text2, from="UTF-8", to="latin1")
db[,]$text3 <- iconv(db[,]$text3, from="UTF-8", to="latin1")
db[,]$text4 <- iconv(db[,]$text4, from="UTF-8", to="latin1")
db[,]$text5 <- iconv(db[,]$text5, from="UTF-8", to="latin1")
db[,]$text6 <- iconv(db[,]$text6, from="UTF-8", to="latin1")
db[,]$text7 <- iconv(db[,]$text7, from="UTF-8", to="latin1")
db[,]$text8 <- iconv(db[,]$text8, from="UTF-8", to="latin1")
db[,]$text9 <- iconv(db[,]$text9, from="UTF-8", to="latin1")
db[,]$text10 <- iconv(db[,]$text10, from="UTF-8", to="latin1")
db$bdtxt1 <- 0; db$bdtxt2 <- 0; db$bdtxt3 <- 0; db$bdtxt4 <- 0; db$bdtxt5 <- 0
db$bdtxt6 <- 0; db$bdtxt7 <- 0; db$bdtxt8 <- 0; db$bdtxt9 <- 0; db$bdtxt10 <- 0
####
for (j in 1:nrow(db)){
w <- unlist(strsplit(tolower(strsplit(db[j,]$text1, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt1 <- wct
#
}
library(readr)
library(stringr)
library(dplyr)
#### to obtain data we can either re generate
# source("gettingdata.R")
#### or we can collect what we have already produced
db <- read_csv('outputs/db.csv')
badwords<-as.vector(paste(readLines("badwords.txt")))
db1 <- db
x <- vector()
list_of_cammnds <- vector()
for (i in 1:10){ # this is terrible for readability ....
x <- paste("db <- mutate(db, reply", i," = !is.na(replyToSN", i, "))", sep="")
list_of_cammnds <- append(list_of_cammnds, x)
eval(parse(text=x))
x <- paste("db <- mutate(db, tweet_location", i," = !is.na(latitude", i, "))", sep="")
eval(parse(text=x))
list_of_cammnds <- append(list_of_cammnds, x)
x <- paste("db <- select(db, -truncated", i, ", -latitude", i, ", -longitude",i,"
, -id",i, ", -created",i, ", -replyToSID",i, ", -replyToSN", i,
", -replyToSID",i, ")"
, sep="")
list_of_cammnds <- append(list_of_cammnds, x)
eval(parse(text=x))
}
db[,]$text1 <- iconv(db[,]$text1, from="UTF-8", to="latin1")
db[,]$text2 <- iconv(db[,]$text2, from="UTF-8", to="latin1")
db[,]$text3 <- iconv(db[,]$text3, from="UTF-8", to="latin1")
db[,]$text4 <- iconv(db[,]$text4, from="UTF-8", to="latin1")
db[,]$text5 <- iconv(db[,]$text5, from="UTF-8", to="latin1")
db[,]$text6 <- iconv(db[,]$text6, from="UTF-8", to="latin1")
db[,]$text7 <- iconv(db[,]$text7, from="UTF-8", to="latin1")
db[,]$text8 <- iconv(db[,]$text8, from="UTF-8", to="latin1")
db[,]$text9 <- iconv(db[,]$text9, from="UTF-8", to="latin1")
db[,]$text10 <- iconv(db[,]$text10, from="UTF-8", to="latin1")
db$bdtxt1 <- 0; db$bdtxt2 <- 0; db$bdtxt3 <- 0; db$bdtxt4 <- 0; db$bdtxt5 <- 0
db$bdtxt6 <- 0; db$bdtxt7 <- 0; db$bdtxt8 <- 0; db$bdtxt9 <- 0; db$bdtxt10 <- 0
####
for (j in 1:nrow(db)){
w <- unlist(strsplit(tolower(strsplit(db[j,]$text1, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt1 <- wct
#
}
db$bdtxt1
for (j in 1:nrow(db)){
w <- unlist(strsplit(tolower(strsplit(db[j,]$text2, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt2 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text3, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt3 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text4, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt4 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text5, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt5 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text6, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt6 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text7, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt7 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text8, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt8 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text9, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt9 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text10, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt10 <- wct
}
db$bdtxt3
db$bdwrd3
# would like a variable that is proportion of letters that are caps #
db$cap1 <- db$cap2 <- db$cap3 <- db$cap4 <- db$cap5 <- db$cap6 <- db$cap7 <- db$cap8 <- 0
db$cap9 <- db$cap10 <- 0
db$cap1 <- db$cap2 <- db$cap3 <- db$cap4 <- db$cap5 <- db$cap6 <- db$cap7 <- db$cap8 <- 0
db$cap9 <- db$cap10 <- 0
# loading the 'excited' vector up... #
excited<-as.vector(paste(readLines("excited.txt")))
# tweetletters <-unlist(strsplit(unlist(strsplit(strsplit(db[64,]$text2, split="\n", fixed = T)[[1]], split = " ")),split=""))
# for (i in 1:length(excited)){
#         print(excited[i])
# }
excited_text_reader <- function(text, dict = excited){
x <- unlist(strsplit(unlist(strsplit(strsplit(text, split="\n", fixed = T)[[1]], split = " ")),split=""))
count <- 0
count = sum(x %in% dict)
return(count)
}
excited_text_reader(db[4, ]$text1)
?sapply()
sapply(db$text1, excited_text_reader)
sapply(db$text1, excited_text_reader)[1]
db$cap1 <- sapply(db$text1, excited_text_reader)
db$cap1
db$cap1 <- sapply(db$text1, excited_text_reader)
db$cap2 <- sapply(db$text2, excited_text_reader)
db$cap3 <- sapply(db$text3, excited_text_reader)
db$cap4 <- sapply(db$text4, excited_text_reader)
db$cap5 <- sapply(db$text5, excited_text_reader)
db$cap6 <- sapply(db$text6, excited_text_reader)
db$cap7 <- sapply(db$text7, excited_text_reader)
db$cap8 <- sapply(db$text8, excited_text_reader)
db$cap9 <- sapply(db$text9, excited_text_reader)
db$cap10 <- sapply(db$text10, excited_text_reader)
?searchTwitter
require(twitteR); require(dplyr)
source("hidden.R")
system('bash set_dir.sh')
# lets look for tweets at hillary clinton
sample_size = 150
tweets <- searchTwitter('@hillaryclinton', n=sample_size); tweets_time <- Sys.time()
# lets generate a list (vector) of all of the observed users
user_names <- vector(length = sample_size)
for (i in 1:sample_size){
user_names[i] <- tweets[[i]]$screenName
}
user_names <- as.vector(user_names)
# lets get info from these users
sample_users <- lookupUsers(user_names)
sample_users_df <- twListToDF(sample_users)
# sample_users_df$screenName # looking at screenNames
# save (write to csv)
write.csv(sample_users_df, "outputs/users.csv")
# lets make a data frame of the tweets we got
tweets_df <- twListToDF(tweets)
#head(tweets_df)
#colnames(tweets_df)
#export this
write.csv(tweets_df, "outputs/tweets.csv")
# making a list of all users and one tweet (not particularly usefull)
tester <- full_join(sample_users_df, tweets_df, by='screenName')
tester <- select(tester, -id.y, -id.x)
#colnames(tester)
#####
#####
#####
#####
#####
# defining this function to allow for easier data frame construction
normalize <- function(w){
w$replyToSN <- as.character(w$replyToSN)
w$replyToSID <- as.character(w$replyToSID)
w$replyToUID <- as.character(w$replyToUID)
w
}
reduce_tweet <- function(x){
x <- select(x, -id, -replyToUID)
x
}
list_dim <- data.frame()
tweet_count <- 20
for (i in 1:length(user_names)){
w <- userTimeline(user_names[i],n=tweet_count, includeRts=TRUE)
if (identical(w, list()) == FALSE){
tempor <- twListToDF(w) # this is a new line...
tempor <- normalize(tempor)
list_dim <- bind_rows(tempor, list_dim)
Sys.sleep(0)
}
}
tweet_observations <- list_dim
# lets say that our data is in list_dim
# we will now attempt to add tweet info to our data rows
tweetno <- 10
print(paste("There are", tweet_count, "tweets per user available."))
print(paste("We will collect the most recent", tweetno))
for (i in 1:tweetno){
if (i == 1){
to_add <- match(unique(tweet_observations$screenName), tweet_observations$screenName)
#tweet_observations <- reduce_tweet(tweet_observations)
add_me <- tweet_observations[to_add,]
colnames(add_me) <- ifelse(colnames(tweet_observations) == "screenName", # TEST
colnames(tweet_observations), paste(colnames(tweet_observations),i,sep=""))# TEST
#                 colnames(add_me) <- ifelse(colnames(add_me) == "screenName",# PRESERVED
#                         colnames(add_me), paste(colnames(add_me),i,sep="")) # PRESERVED
db <- full_join(sample_users_df, add_me, by='screenName') #changing tweet_observations[to_add,] to add_me
} else {
# removing those observations already added
tweet_observations <- tweet_observations[-to_add,]
# finding new unique observations to add
to_add <- match(unique(tweet_observations$screenName), tweet_observations$screenName)
# storing these
add_me <- tweet_observations[to_add,]
# renaming
colnames(add_me) <- ifelse(colnames(tweet_observations) == "screenName",
colnames(tweet_observations), paste(colnames(tweet_observations),i,sep=""))
db <- full_join(db, add_me, by='screenName')
}
}
# colnames(db)
bu <- db
write.csv(db, "outputs/db.csv")
paste("Data was generated from twitter on", tweets_time)
library(readr)
library(stringr)
library(dplyr)
#### to obtain data we can either re generate
# source("gettingdata.R")
#### or we can collect what we have already produced
db <- read_csv('outputs/db.csv')
badwords<-as.vector(paste(readLines("badwords.txt")))
db1 <- db
x <- vector()
list_of_cammnds <- vector()
for (i in 1:10){ # this is terrible for readability ....
x <- paste("db <- mutate(db, reply", i," = !is.na(replyToSN", i, "))", sep="")
list_of_cammnds <- append(list_of_cammnds, x)
eval(parse(text=x))
x <- paste("db <- mutate(db, tweet_location", i," = !is.na(latitude", i, "))", sep="")
eval(parse(text=x))
list_of_cammnds <- append(list_of_cammnds, x)
x <- paste("db <- select(db, -truncated", i, ", -latitude", i, ", -longitude",i,"
, -id",i, ", -created",i, ", -replyToSID",i, ", -replyToSN", i,
", -replyToSID",i, ")"
, sep="")
list_of_cammnds <- append(list_of_cammnds, x)
eval(parse(text=x))
}
db[,]$text1 <- iconv(db[,]$text1, from="UTF-8", to="latin1")
db[,]$text2 <- iconv(db[,]$text2, from="UTF-8", to="latin1")
db[,]$text3 <- iconv(db[,]$text3, from="UTF-8", to="latin1")
db[,]$text4 <- iconv(db[,]$text4, from="UTF-8", to="latin1")
db[,]$text5 <- iconv(db[,]$text5, from="UTF-8", to="latin1")
db[,]$text6 <- iconv(db[,]$text6, from="UTF-8", to="latin1")
db[,]$text7 <- iconv(db[,]$text7, from="UTF-8", to="latin1")
db[,]$text8 <- iconv(db[,]$text8, from="UTF-8", to="latin1")
db[,]$text9 <- iconv(db[,]$text9, from="UTF-8", to="latin1")
db[,]$text10 <- iconv(db[,]$text10, from="UTF-8", to="latin1")
db$bdtxt1 <- 0; db$bdtxt2 <- 0; db$bdtxt3 <- 0; db$bdtxt4 <- 0; db$bdtxt5 <- 0
db$bdtxt6 <- 0; db$bdtxt7 <- 0; db$bdtxt8 <- 0; db$bdtxt9 <- 0; db$bdtxt10 <- 0
####
for (j in 1:nrow(db)){
w <- unlist(strsplit(tolower(strsplit(db[j,]$text1, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt1 <- wct
#
}
db$bdtxt1
for (j in 1:nrow(db)){
w <- unlist(strsplit(tolower(strsplit(db[j,]$text2, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt2 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text3, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt3 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text4, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt4 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text5, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt5 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text6, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt6 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text7, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt7 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text8, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt8 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text9, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt9 <- wct
#
w <- unlist(strsplit(tolower(strsplit(db[j,]$text10, split="\n", fixed = T)[[1]]), split = " "))
wct <- 0
for (i in 1:length(w)){
wct <- wct + w[i] %in% badwords
}
db[j,]$bdtxt10 <- wct
}
# would like a variable that is proportion of letters that are caps #
db$cap1 <- db$cap2 <- db$cap3 <- db$cap4 <- db$cap5 <- db$cap6 <- db$cap7 <- db$cap8 <- 0
db$cap9 <- db$cap10 <- 0
# loading the 'excited' vector up... #
excited<-as.vector(paste(readLines("excited.txt")))
# tweetletters <-unlist(strsplit(unlist(strsplit(strsplit(db[64,]$text2, split="\n", fixed = T)[[1]], split = " ")),split=""))
# for (i in 1:length(excited)){
#         print(excited[i])
# }
excited_text_reader <- function(text, dict = excited){
x <- unlist(strsplit(unlist(strsplit(strsplit(text, split="\n", fixed = T)[[1]], split = " ")),split=""))
count <- 0
count = sum(x %in% dict)
return(count)
}
db$cap1 <- sapply(db$text1, excited_text_reader)
db$cap2 <- sapply(db$text2, excited_text_reader)
db$cap3 <- sapply(db$text3, excited_text_reader)
db$cap4 <- sapply(db$text4, excited_text_reader)
db$cap5 <- sapply(db$text5, excited_text_reader)
db$cap6 <- sapply(db$text6, excited_text_reader)
db$cap7 <- sapply(db$text7, excited_text_reader)
db$cap8 <- sapply(db$text8, excited_text_reader)
db$cap9 <- sapply(db$text9, excited_text_reader)
db$cap10 <- sapply(db$text10, excited_text_reader)
#
#
colnames(db)
str_count(tolower(db[,]$text1), "clinton")
write.csv(for_lab, "process.csv")
write.csv(db, "processed.csv")
xpathobs <- "/html/body/div[4]/table"
xpathobs <- "/html/body/div[4]/table"
table_xpath <- "/html/body/div[4]/table"
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
install.packages("rvest")
library(rvest)
htmlfile = read_html(table_url)
table_xpath <- "/html/body/div[4]/table"
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
install.packages("rvest")
install.packages("rvest")
table_xpath <- "/html/body/div[4]/table"
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
library(rvest)
htmlfile = read_html(table_url)
nds = html_nodes(table_xpath,
xpath=table_xpath)
nds = html_nodes(table_xpath,
xpath=table_xpath)
nds = html_nodes(table_xpath,
xpath="/html/body/div[4]/table")
nds = html_nodes(table_xpath
)
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
population <- url %>%
html() %>%
html_nodes(xpath='/html/body/div[4]/table') %>%
html_table()
library(dplyr)
htmlfile = read_html(table_url)
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
population <- table_url %>%
html() %>%
html_nodes(xpath='/html/body/div[4]/table[1]') %>%
html_table()
help("Deprecated")
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
population <- table_url %>%
read_html() %>%
html_nodes(xpath='/html/body/div[4]/table[1]') %>%
html_table()
population <- population[[1]]
population
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
population <- table_url %>%
read_html() %>%
html_nodes(xpath='/html/body/div[4]/table[1]') %>%
html_table()
population
table_url
population <- table_url %>%
read_html() %>%
html_nodes(xpath='/html/body/div[4]/table') %>%
html_table()
population
unclass(population)
str(population)
unlist(population)
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
htmlfile = read_html(table_url)
htmlfile
str(htmlfile)
library(rvest)
recount_url = "http://bowtie-bio.sourceforge.net/recount/"
htmlfile = read_html(recount_url)
nds = html_nodes(htmlfile,
xpath='//*[@id="recounttab"]/table')
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
population <- table_url %>%
read_html() %>%
html_nodes(xpath='/html/body/div[3]/table') %>%
html_table()
population
colnames(population)
population <- table_url %>%
read_html() %>%
html_nodes(xpath='/html/body/div[3]/table') %>%
html_table() %>% as.data.frame()
colnames(population)
head(population)
View(population)
population$X[1]
population$X[2]
population$X[1:20]
population$X[1:21]
population <- population[!(population$X. == "№"), ]
View(population)
population[1,]
)
colnames(population)
head(population)
# getting rid of weird headings in the middle of the dataframe...
population <- population[!(population$X. == "№"), ]
population <- select(population, -Chart, -Apple, -Goog., -Twtr., -One, -FBM, -Wind., -Sams., -GMail,
-SB, -DCM, -KDDI, -Name)
library(rvest)
library(dplyr)
table_url = "http://unicode.org/emoji/charts/full-emoji-list.html"
population <- table_url %>%
read_html() %>%
html_nodes(xpath='/html/body/div[3]/table') %>%
html_table() %>% as.data.frame()
head(population)
head(population)
# getting rid of weird headings in the middle of the dataframe...
population <- population[!(population$X. == "№"), ]
population <- select(population, -Chart, -Apple, -Goog., -Twtr., -One, -FBM, -Wind., -Sams., -GMail,
-SB, -DCM, -KDDI, -Name)
colnames(population)
head(population)
